{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inySffA0rVh8"
   },
   "source": [
    "## Кривоногов Н.В., PyTorch, практическое задание № 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxrTT1LFrVh9"
   },
   "source": [
    "Будем практиковаться на датасете недвижимости (sklearn.datasets.fetch_california_housing)\n",
    "\n",
    "Ваша задача:\n",
    "1. Создать Dataset для загрузки данных\n",
    "2. Обернуть его в Dataloader\n",
    "3. Написать архитектуру сети, которая предсказывает стоимость недвижимости. Сеть должна включать BatchNorm слои и Dropout (или НЕ включать, но нужно обосновать)\n",
    "4. Сравните сходимость Adam, RMSProp и SGD, сделайте вывод по качеству работы модели\n",
    "\n",
    "train-test разделение нужно сделать с помощью sklearn random_state=13, test_size = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cAm5EVoLrVh-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка датасета и разбиение на обучающую и тестовую выборку: \n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание класса Dataset: \n",
    "\n",
    "class MyOwnFCH(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = torch.from_numpy(X_data).type(torch.float)\n",
    "        self.y_data = torch.from_numpy(y_data).type(torch.float)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание классов нейросети: \n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        if self.activation == \"relu\":\n",
    "            return F.relu(x)\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return F.sigmoid(x)\n",
    "        raise RuntimeError\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = Perceptron(input_dim, hidden_dim)\n",
    "        self.dp = nn.Dropout(0.25)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = Perceptron(hidden_dim, 1, \"relu\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dp(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание экземпляров класса Dataset: \n",
    "\n",
    "train_dataset = MyOwnFCH(X_train, y_train)\n",
    "\n",
    "test_dataset = MyOwnFCH(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание экземпляров класса DataLoader: \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForward(\n",
       "  (fc1): Perceptron(\n",
       "    (fc): Linear(in_features=8, out_features=8, bias=True)\n",
       "  )\n",
       "  (dp): Dropout(p=0.25, inplace=False)\n",
       "  (bn): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Perceptron(\n",
       "    (fc): Linear(in_features=8, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# инициализация нейросети: \n",
    "\n",
    "net = FeedForward(8, 8)\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция потерь для задачи регрессии: \n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Оптимизатор:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Обучение: \n",
      "Epoch [1/20]. Step [1/121]. Loss: 0.033. Train r2: -2.179. Test r2: -4.036\n",
      "Epoch [1/20]. Step [41/121]. Loss: 0.034. Train r2: -97.035. Test r2: -3.319\n",
      "Epoch [1/20]. Step [81/121]. Loss: 0.034. Train r2: -93.575. Test r2: -3.341\n",
      "Epoch [1/20]. Step [121/121]. Loss: 0.035. Train r2: -94.435. Test r2: -3.223\n",
      "Epoch [2/20]. Step [1/121]. Loss: 0.030. Train r2: -2.322. Test r2: -3.235\n",
      "Epoch [2/20]. Step [41/121]. Loss: 0.034. Train r2: -89.967. Test r2: -3.192\n",
      "Epoch [2/20]. Step [81/121]. Loss: 0.035. Train r2: -91.631. Test r2: -3.187\n",
      "Epoch [2/20]. Step [121/121]. Loss: 0.032. Train r2: -91.497. Test r2: -3.137\n",
      "Epoch [3/20]. Step [1/121]. Loss: 0.039. Train r2: -2.472. Test r2: -3.124\n",
      "Epoch [3/20]. Step [41/121]. Loss: 0.033. Train r2: -88.524. Test r2: -3.056\n",
      "Epoch [3/20]. Step [81/121]. Loss: 0.032. Train r2: -86.437. Test r2: -3.055\n",
      "Epoch [3/20]. Step [121/121]. Loss: 0.032. Train r2: -84.574. Test r2: -3.058\n",
      "Epoch [4/20]. Step [1/121]. Loss: 0.033. Train r2: -1.784. Test r2: -3.059\n",
      "Epoch [4/20]. Step [41/121]. Loss: 0.032. Train r2: -86.041. Test r2: -3.017\n",
      "Epoch [4/20]. Step [81/121]. Loss: 0.031. Train r2: -84.828. Test r2: -2.916\n",
      "Epoch [4/20]. Step [121/121]. Loss: 0.031. Train r2: -78.022. Test r2: -2.883\n",
      "Epoch [5/20]. Step [1/121]. Loss: 0.030. Train r2: -2.078. Test r2: -2.884\n",
      "Epoch [5/20]. Step [41/121]. Loss: 0.030. Train r2: -82.006. Test r2: -2.852\n",
      "Epoch [5/20]. Step [81/121]. Loss: 0.031. Train r2: -79.291. Test r2: -2.884\n",
      "Epoch [5/20]. Step [121/121]. Loss: 0.031. Train r2: -74.715. Test r2: -2.779\n",
      "Epoch [6/20]. Step [1/121]. Loss: 0.025. Train r2: -1.823. Test r2: -2.757\n",
      "Epoch [6/20]. Step [41/121]. Loss: 0.029. Train r2: -73.956. Test r2: -2.707\n",
      "Epoch [6/20]. Step [81/121]. Loss: 0.030. Train r2: -75.749. Test r2: -2.754\n",
      "Epoch [6/20]. Step [121/121]. Loss: 0.029. Train r2: -73.792. Test r2: -2.702\n",
      "Epoch [7/20]. Step [1/121]. Loss: 0.032. Train r2: -1.810. Test r2: -2.687\n",
      "Epoch [7/20]. Step [41/121]. Loss: 0.028. Train r2: -71.071. Test r2: -2.557\n",
      "Epoch [7/20]. Step [81/121]. Loss: 0.029. Train r2: -68.800. Test r2: -2.602\n",
      "Epoch [7/20]. Step [121/121]. Loss: 0.028. Train r2: -69.282. Test r2: -2.521\n",
      "Epoch [8/20]. Step [1/121]. Loss: 0.026. Train r2: -1.837. Test r2: -2.520\n",
      "Epoch [8/20]. Step [41/121]. Loss: 0.028. Train r2: -67.065. Test r2: -2.495\n",
      "Epoch [8/20]. Step [81/121]. Loss: 0.028. Train r2: -66.793. Test r2: -2.410\n",
      "Epoch [8/20]. Step [121/121]. Loss: 0.027. Train r2: -63.290. Test r2: -2.445\n",
      "Epoch [9/20]. Step [1/121]. Loss: 0.024. Train r2: -2.041. Test r2: -2.450\n",
      "Epoch [9/20]. Step [41/121]. Loss: 0.026. Train r2: -62.002. Test r2: -2.347\n",
      "Epoch [9/20]. Step [81/121]. Loss: 0.027. Train r2: -61.049. Test r2: -2.369\n",
      "Epoch [9/20]. Step [121/121]. Loss: 0.026. Train r2: -60.494. Test r2: -2.334\n",
      "Epoch [10/20]. Step [1/121]. Loss: 0.024. Train r2: -1.540. Test r2: -2.339\n",
      "Epoch [10/20]. Step [41/121]. Loss: 0.026. Train r2: -57.662. Test r2: -2.116\n",
      "Epoch [10/20]. Step [81/121]. Loss: 0.025. Train r2: -55.502. Test r2: -2.027\n",
      "Epoch [10/20]. Step [121/121]. Loss: 0.024. Train r2: -54.355. Test r2: -1.924\n",
      "Epoch [11/20]. Step [1/121]. Loss: 0.029. Train r2: -1.178. Test r2: -1.913\n",
      "Epoch [11/20]. Step [41/121]. Loss: 0.024. Train r2: -53.034. Test r2: -1.967\n",
      "Epoch [11/20]. Step [81/121]. Loss: 0.024. Train r2: -49.645. Test r2: -1.920\n",
      "Epoch [11/20]. Step [121/121]. Loss: 0.022. Train r2: -49.081. Test r2: -1.745\n",
      "Epoch [12/20]. Step [1/121]. Loss: 0.021. Train r2: -1.121. Test r2: -1.741\n",
      "Epoch [12/20]. Step [41/121]. Loss: 0.022. Train r2: -46.615. Test r2: -1.442\n",
      "Epoch [12/20]. Step [81/121]. Loss: 0.022. Train r2: -45.780. Test r2: -1.359\n",
      "Epoch [12/20]. Step [121/121]. Loss: 0.021. Train r2: -40.633. Test r2: -1.310\n",
      "Epoch [13/20]. Step [1/121]. Loss: 0.025. Train r2: -1.148. Test r2: -1.289\n",
      "Epoch [13/20]. Step [41/121]. Loss: 0.021. Train r2: -41.306. Test r2: -1.265\n",
      "Epoch [13/20]. Step [81/121]. Loss: 0.020. Train r2: -39.002. Test r2: -1.205\n",
      "Epoch [13/20]. Step [121/121]. Loss: 0.020. Train r2: -38.936. Test r2: -1.243\n",
      "Epoch [14/20]. Step [1/121]. Loss: 0.019. Train r2: -0.975. Test r2: -1.229\n",
      "Epoch [14/20]. Step [41/121]. Loss: 0.020. Train r2: -36.162. Test r2: -0.970\n",
      "Epoch [14/20]. Step [81/121]. Loss: 0.020. Train r2: -36.111. Test r2: -0.638\n",
      "Epoch [14/20]. Step [121/121]. Loss: 0.019. Train r2: -34.252. Test r2: -0.939\n",
      "Epoch [15/20]. Step [1/121]. Loss: 0.018. Train r2: -0.661. Test r2: -0.958\n",
      "Epoch [15/20]. Step [41/121]. Loss: 0.019. Train r2: -32.850. Test r2: -0.713\n",
      "Epoch [15/20]. Step [81/121]. Loss: 0.019. Train r2: -32.232. Test r2: -0.632\n",
      "Epoch [15/20]. Step [121/121]. Loss: 0.019. Train r2: -30.111. Test r2: -0.766\n",
      "Epoch [16/20]. Step [1/121]. Loss: 0.015. Train r2: -0.620. Test r2: -0.682\n",
      "Epoch [16/20]. Step [41/121]. Loss: 0.018. Train r2: -29.721. Test r2: -0.454\n",
      "Epoch [16/20]. Step [81/121]. Loss: 0.018. Train r2: -27.435. Test r2: -0.500\n",
      "Epoch [16/20]. Step [121/121]. Loss: 0.017. Train r2: -26.606. Test r2: -0.473\n",
      "Epoch [17/20]. Step [1/121]. Loss: 0.011. Train r2: -0.456. Test r2: -0.471\n",
      "Epoch [17/20]. Step [41/121]. Loss: 0.017. Train r2: -25.561. Test r2: -0.460\n",
      "Epoch [17/20]. Step [81/121]. Loss: 0.017. Train r2: -23.724. Test r2: -0.361\n",
      "Epoch [17/20]. Step [121/121]. Loss: 0.016. Train r2: -22.787. Test r2: -0.163\n",
      "Epoch [18/20]. Step [1/121]. Loss: 0.018. Train r2: -0.429. Test r2: -0.150\n",
      "Epoch [18/20]. Step [41/121]. Loss: 0.016. Train r2: -20.716. Test r2: -0.082\n",
      "Epoch [18/20]. Step [81/121]. Loss: 0.015. Train r2: -19.292. Test r2: -0.124\n",
      "Epoch [18/20]. Step [121/121]. Loss: 0.015. Train r2: -18.147. Test r2: -0.077\n",
      "Epoch [19/20]. Step [1/121]. Loss: 0.012. Train r2: -0.387. Test r2: -0.077\n",
      "Epoch [19/20]. Step [41/121]. Loss: 0.015. Train r2: -17.428. Test r2: -0.052\n",
      "Epoch [19/20]. Step [81/121]. Loss: 0.015. Train r2: -16.587. Test r2: -0.035\n",
      "Epoch [19/20]. Step [121/121]. Loss: 0.014. Train r2: -15.193. Test r2: -0.082\n",
      "Epoch [20/20]. Step [1/121]. Loss: 0.015. Train r2: -0.438. Test r2: -0.085\n",
      "Epoch [20/20]. Step [41/121]. Loss: 0.014. Train r2: -14.888. Test r2: -0.073\n",
      "Epoch [20/20]. Step [81/121]. Loss: 0.014. Train r2: -14.071. Test r2: -0.033\n",
      "Epoch [20/20]. Step [121/121]. Loss: 0.014. Train r2: -13.522. Test r2: -0.012\n",
      "Обучение закончено!\n",
      "_________________________________\n",
      "Оптимизатор:  RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "Обучение: \n",
      "Epoch [1/20]. Step [1/121]. Loss: 0.011. Train r2: -0.379. Test r2: -0.007\n",
      "Epoch [1/20]. Step [41/121]. Loss: 0.013. Train r2: -10.186. Test r2: -0.005\n",
      "Epoch [1/20]. Step [81/121]. Loss: 0.013. Train r2: -9.666. Test r2: -0.000\n",
      "Epoch [1/20]. Step [121/121]. Loss: 0.013. Train r2: -8.944. Test r2: -0.003\n",
      "Epoch [2/20]. Step [1/121]. Loss: 0.012. Train r2: -0.208. Test r2: -0.003\n",
      "Epoch [2/20]. Step [41/121]. Loss: 0.012. Train r2: -7.756. Test r2: -0.014\n",
      "Epoch [2/20]. Step [81/121]. Loss: 0.012. Train r2: -6.910. Test r2: -0.011\n",
      "Epoch [2/20]. Step [121/121]. Loss: 0.012. Train r2: -6.922. Test r2: -0.035\n",
      "Epoch [3/20]. Step [1/121]. Loss: 0.014. Train r2: -0.236. Test r2: -0.030\n",
      "Epoch [3/20]. Step [41/121]. Loss: 0.012. Train r2: -5.956. Test r2: -0.029\n",
      "Epoch [3/20]. Step [81/121]. Loss: 0.012. Train r2: -5.535. Test r2: -0.029\n",
      "Epoch [3/20]. Step [121/121]. Loss: 0.012. Train r2: -5.221. Test r2: -0.060\n",
      "Epoch [4/20]. Step [1/121]. Loss: 0.010. Train r2: -0.181. Test r2: -0.056\n",
      "Epoch [4/20]. Step [41/121]. Loss: 0.012. Train r2: -5.044. Test r2: -0.070\n",
      "Epoch [4/20]. Step [81/121]. Loss: 0.011. Train r2: -4.517. Test r2: -0.101\n",
      "Epoch [4/20]. Step [121/121]. Loss: 0.012. Train r2: -4.410. Test r2: -0.086\n",
      "Epoch [5/20]. Step [1/121]. Loss: 0.013. Train r2: -0.083. Test r2: -0.094\n",
      "Epoch [5/20]. Step [41/121]. Loss: 0.011. Train r2: -3.978. Test r2: -0.110\n",
      "Epoch [5/20]. Step [81/121]. Loss: 0.012. Train r2: -3.983. Test r2: -0.119\n",
      "Epoch [5/20]. Step [121/121]. Loss: 0.011. Train r2: -2.989. Test r2: -0.100\n",
      "Epoch [6/20]. Step [1/121]. Loss: 0.012. Train r2: -0.087. Test r2: -0.101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20]. Step [41/121]. Loss: 0.011. Train r2: -2.674. Test r2: -0.088\n",
      "Epoch [6/20]. Step [81/121]. Loss: 0.011. Train r2: -3.032. Test r2: -0.141\n",
      "Epoch [6/20]. Step [121/121]. Loss: 0.011. Train r2: -3.230. Test r2: -0.142\n",
      "Epoch [7/20]. Step [1/121]. Loss: 0.009. Train r2: -0.090. Test r2: -0.141\n",
      "Epoch [7/20]. Step [41/121]. Loss: 0.011. Train r2: -2.851. Test r2: -0.154\n",
      "Epoch [7/20]. Step [81/121]. Loss: 0.011. Train r2: -2.412. Test r2: -0.167\n",
      "Epoch [7/20]. Step [121/121]. Loss: 0.011. Train r2: -2.045. Test r2: -0.169\n",
      "Epoch [8/20]. Step [1/121]. Loss: 0.009. Train r2: -0.030. Test r2: -0.171\n",
      "Epoch [8/20]. Step [41/121]. Loss: 0.011. Train r2: -2.214. Test r2: -0.190\n",
      "Epoch [8/20]. Step [81/121]. Loss: 0.011. Train r2: -2.093. Test r2: -0.178\n",
      "Epoch [8/20]. Step [121/121]. Loss: 0.011. Train r2: -1.617. Test r2: -0.225\n",
      "Epoch [9/20]. Step [1/121]. Loss: 0.010. Train r2: -0.010. Test r2: -0.231\n",
      "Epoch [9/20]. Step [41/121]. Loss: 0.011. Train r2: -1.828. Test r2: -0.249\n",
      "Epoch [9/20]. Step [81/121]. Loss: 0.011. Train r2: -1.731. Test r2: -0.233\n",
      "Epoch [9/20]. Step [121/121]. Loss: 0.011. Train r2: -1.596. Test r2: -0.261\n",
      "Epoch [10/20]. Step [1/121]. Loss: 0.012. Train r2: -0.014. Test r2: -0.285\n",
      "Epoch [10/20]. Step [41/121]. Loss: 0.010. Train r2: -1.457. Test r2: -0.263\n",
      "Epoch [10/20]. Step [81/121]. Loss: 0.011. Train r2: -1.490. Test r2: -0.259\n",
      "Epoch [10/20]. Step [121/121]. Loss: 0.011. Train r2: -0.891. Test r2: -0.263\n",
      "Epoch [11/20]. Step [1/121]. Loss: 0.012. Train r2: -0.020. Test r2: -0.262\n",
      "Epoch [11/20]. Step [41/121]. Loss: 0.011. Train r2: -1.221. Test r2: -0.286\n",
      "Epoch [11/20]. Step [81/121]. Loss: 0.010. Train r2: -0.478. Test r2: -0.259\n",
      "Epoch [11/20]. Step [121/121]. Loss: 0.011. Train r2: -0.628. Test r2: -0.277\n",
      "Epoch [12/20]. Step [1/121]. Loss: 0.011. Train r2: -0.001. Test r2: -0.276\n",
      "Epoch [12/20]. Step [41/121]. Loss: 0.010. Train r2: -0.707. Test r2: -0.337\n",
      "Epoch [12/20]. Step [81/121]. Loss: 0.011. Train r2: -0.501. Test r2: -0.296\n",
      "Epoch [12/20]. Step [121/121]. Loss: 0.010. Train r2: -0.510. Test r2: -0.291\n",
      "Epoch [13/20]. Step [1/121]. Loss: 0.010. Train r2: -0.001. Test r2: -0.291\n",
      "Epoch [13/20]. Step [41/121]. Loss: 0.011. Train r2: -0.494. Test r2: -0.317\n",
      "Epoch [13/20]. Step [81/121]. Loss: 0.010. Train r2: -0.462. Test r2: -0.331\n",
      "Epoch [13/20]. Step [121/121]. Loss: 0.010. Train r2: -0.333. Test r2: -0.303\n",
      "Epoch [14/20]. Step [1/121]. Loss: 0.008. Train r2: -0.005. Test r2: -0.303\n",
      "Epoch [14/20]. Step [41/121]. Loss: 0.011. Train r2: -0.338. Test r2: -0.336\n",
      "Epoch [14/20]. Step [81/121]. Loss: 0.011. Train r2: -0.597. Test r2: -0.330\n",
      "Epoch [14/20]. Step [121/121]. Loss: 0.010. Train r2: -0.393. Test r2: -0.318\n",
      "Epoch [15/20]. Step [1/121]. Loss: 0.010. Train r2: -0.026. Test r2: -0.319\n",
      "Epoch [15/20]. Step [41/121]. Loss: 0.011. Train r2: -0.524. Test r2: -0.333\n",
      "Epoch [15/20]. Step [81/121]. Loss: 0.011. Train r2: -0.328. Test r2: -0.331\n",
      "Epoch [15/20]. Step [121/121]. Loss: 0.010. Train r2: -0.459. Test r2: -0.318\n",
      "Epoch [16/20]. Step [1/121]. Loss: 0.013. Train r2: -0.001. Test r2: -0.338\n",
      "Epoch [16/20]. Step [41/121]. Loss: 0.011. Train r2: -0.583. Test r2: -0.314\n",
      "Epoch [16/20]. Step [81/121]. Loss: 0.010. Train r2: -0.517. Test r2: -0.336\n",
      "Epoch [16/20]. Step [121/121]. Loss: 0.010. Train r2: -0.257. Test r2: -0.312\n",
      "Epoch [17/20]. Step [1/121]. Loss: 0.010. Train r2: -0.001. Test r2: -0.312\n",
      "Epoch [17/20]. Step [41/121]. Loss: 0.010. Train r2: -0.391. Test r2: -0.328\n",
      "Epoch [17/20]. Step [81/121]. Loss: 0.010. Train r2: -0.514. Test r2: -0.316\n",
      "Epoch [17/20]. Step [121/121]. Loss: 0.011. Train r2: -0.473. Test r2: -0.329\n",
      "Epoch [18/20]. Step [1/121]. Loss: 0.011. Train r2: -0.001. Test r2: -0.328\n",
      "Epoch [18/20]. Step [41/121]. Loss: 0.011. Train r2: -0.606. Test r2: -0.356\n",
      "Epoch [18/20]. Step [81/121]. Loss: 0.010. Train r2: -0.538. Test r2: -0.313\n",
      "Epoch [18/20]. Step [121/121]. Loss: 0.010. Train r2: -0.252. Test r2: -0.307\n",
      "Epoch [19/20]. Step [1/121]. Loss: 0.009. Train r2: -0.000. Test r2: -0.307\n",
      "Epoch [19/20]. Step [41/121]. Loss: 0.011. Train r2: -0.553. Test r2: -0.338\n",
      "Epoch [19/20]. Step [81/121]. Loss: 0.010. Train r2: -0.342. Test r2: -0.315\n",
      "Epoch [19/20]. Step [121/121]. Loss: 0.010. Train r2: -0.560. Test r2: -0.322\n",
      "Epoch [20/20]. Step [1/121]. Loss: 0.010. Train r2: -0.009. Test r2: -0.321\n",
      "Epoch [20/20]. Step [41/121]. Loss: 0.010. Train r2: -0.513. Test r2: -0.343\n",
      "Epoch [20/20]. Step [81/121]. Loss: 0.010. Train r2: -0.489. Test r2: -0.342\n",
      "Epoch [20/20]. Step [121/121]. Loss: 0.011. Train r2: -0.285. Test r2: -0.311\n",
      "Обучение закончено!\n",
      "_________________________________\n",
      "Оптимизатор:  SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Обучение: \n",
      "Epoch [1/20]. Step [1/121]. Loss: 0.009. Train r2: -0.005. Test r2: -0.311\n",
      "Epoch [1/20]. Step [41/121]. Loss: 0.011. Train r2: -0.455. Test r2: -0.324\n",
      "Epoch [1/20]. Step [81/121]. Loss: 0.010. Train r2: -0.285. Test r2: -0.310\n",
      "Epoch [1/20]. Step [121/121]. Loss: 0.010. Train r2: -0.327. Test r2: -0.325\n",
      "Epoch [2/20]. Step [1/121]. Loss: 0.007. Train r2: -0.039. Test r2: -0.324\n",
      "Epoch [2/20]. Step [41/121]. Loss: 0.010. Train r2: -0.317. Test r2: -0.342\n",
      "Epoch [2/20]. Step [81/121]. Loss: 0.011. Train r2: -0.378. Test r2: -0.314\n",
      "Epoch [2/20]. Step [121/121]. Loss: 0.011. Train r2: -0.328. Test r2: -0.328\n",
      "Epoch [3/20]. Step [1/121]. Loss: 0.008. Train r2: -0.008. Test r2: -0.327\n",
      "Epoch [3/20]. Step [41/121]. Loss: 0.011. Train r2: -0.361. Test r2: -0.325\n",
      "Epoch [3/20]. Step [81/121]. Loss: 0.010. Train r2: -0.355. Test r2: -0.320\n",
      "Epoch [3/20]. Step [121/121]. Loss: 0.010. Train r2: -0.352. Test r2: -0.318\n",
      "Epoch [4/20]. Step [1/121]. Loss: 0.010. Train r2: -0.000. Test r2: -0.318\n",
      "Epoch [4/20]. Step [41/121]. Loss: 0.010. Train r2: -0.620. Test r2: -0.316\n",
      "Epoch [4/20]. Step [81/121]. Loss: 0.010. Train r2: -0.290. Test r2: -0.313\n",
      "Epoch [4/20]. Step [121/121]. Loss: 0.011. Train r2: -0.297. Test r2: -0.335\n",
      "Epoch [5/20]. Step [1/121]. Loss: 0.011. Train r2: -0.001. Test r2: -0.334\n",
      "Epoch [5/20]. Step [41/121]. Loss: 0.010. Train r2: -0.287. Test r2: -0.319\n",
      "Epoch [5/20]. Step [81/121]. Loss: 0.010. Train r2: -0.198. Test r2: -0.311\n",
      "Epoch [5/20]. Step [121/121]. Loss: 0.011. Train r2: -0.375. Test r2: -0.338\n",
      "Epoch [6/20]. Step [1/121]. Loss: 0.011. Train r2: -0.005. Test r2: -0.336\n",
      "Epoch [6/20]. Step [41/121]. Loss: 0.010. Train r2: -0.460. Test r2: -0.336\n",
      "Epoch [6/20]. Step [81/121]. Loss: 0.010. Train r2: -0.516. Test r2: -0.349\n",
      "Epoch [6/20]. Step [121/121]. Loss: 0.011. Train r2: -0.309. Test r2: -0.314\n",
      "Epoch [7/20]. Step [1/121]. Loss: 0.010. Train r2: -0.011. Test r2: -0.314\n",
      "Epoch [7/20]. Step [41/121]. Loss: 0.010. Train r2: -0.378. Test r2: -0.320\n",
      "Epoch [7/20]. Step [81/121]. Loss: 0.010. Train r2: -0.326. Test r2: -0.311\n",
      "Epoch [7/20]. Step [121/121]. Loss: 0.010. Train r2: -0.296. Test r2: -0.321\n",
      "Epoch [8/20]. Step [1/121]. Loss: 0.011. Train r2: -0.002. Test r2: -0.321\n",
      "Epoch [8/20]. Step [41/121]. Loss: 0.011. Train r2: -0.291. Test r2: -0.326\n",
      "Epoch [8/20]. Step [81/121]. Loss: 0.010. Train r2: -0.331. Test r2: -0.318\n",
      "Epoch [8/20]. Step [121/121]. Loss: 0.010. Train r2: -0.410. Test r2: -0.336\n",
      "Epoch [9/20]. Step [1/121]. Loss: 0.010. Train r2: -0.000. Test r2: -0.335\n",
      "Epoch [9/20]. Step [41/121]. Loss: 0.011. Train r2: -0.354. Test r2: -0.317\n",
      "Epoch [9/20]. Step [81/121]. Loss: 0.011. Train r2: -0.265. Test r2: -0.319\n",
      "Epoch [9/20]. Step [121/121]. Loss: 0.010. Train r2: -0.360. Test r2: -0.311\n",
      "Epoch [10/20]. Step [1/121]. Loss: 0.010. Train r2: -0.000. Test r2: -0.311\n",
      "Epoch [10/20]. Step [41/121]. Loss: 0.010. Train r2: -0.342. Test r2: -0.324\n",
      "Epoch [10/20]. Step [81/121]. Loss: 0.011. Train r2: -0.238. Test r2: -0.316\n",
      "Epoch [10/20]. Step [121/121]. Loss: 0.010. Train r2: -0.275. Test r2: -0.311\n",
      "Epoch [11/20]. Step [1/121]. Loss: 0.009. Train r2: -0.001. Test r2: -0.311\n",
      "Epoch [11/20]. Step [41/121]. Loss: 0.010. Train r2: -0.371. Test r2: -0.314\n",
      "Epoch [11/20]. Step [81/121]. Loss: 0.010. Train r2: -0.299. Test r2: -0.322\n",
      "Epoch [11/20]. Step [121/121]. Loss: 0.010. Train r2: -0.367. Test r2: -0.325\n",
      "Epoch [12/20]. Step [1/121]. Loss: 0.011. Train r2: -0.001. Test r2: -0.324\n",
      "Epoch [12/20]. Step [41/121]. Loss: 0.010. Train r2: -0.353. Test r2: -0.325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20]. Step [81/121]. Loss: 0.011. Train r2: -0.256. Test r2: -0.312\n",
      "Epoch [12/20]. Step [121/121]. Loss: 0.010. Train r2: -0.408. Test r2: -0.332\n",
      "Epoch [13/20]. Step [1/121]. Loss: 0.009. Train r2: 0.008. Test r2: -0.339\n",
      "Epoch [13/20]. Step [41/121]. Loss: 0.011. Train r2: -0.230. Test r2: -0.316\n",
      "Epoch [13/20]. Step [81/121]. Loss: 0.010. Train r2: -0.405. Test r2: -0.336\n",
      "Epoch [13/20]. Step [121/121]. Loss: 0.010. Train r2: -0.254. Test r2: -0.321\n",
      "Epoch [14/20]. Step [1/121]. Loss: 0.011. Train r2: -0.000. Test r2: -0.320\n",
      "Epoch [14/20]. Step [41/121]. Loss: 0.010. Train r2: -0.402. Test r2: -0.329\n",
      "Epoch [14/20]. Step [81/121]. Loss: 0.010. Train r2: -0.245. Test r2: -0.314\n",
      "Epoch [14/20]. Step [121/121]. Loss: 0.010. Train r2: -0.411. Test r2: -0.316\n",
      "Epoch [15/20]. Step [1/121]. Loss: 0.009. Train r2: -0.002. Test r2: -0.316\n",
      "Epoch [15/20]. Step [41/121]. Loss: 0.011. Train r2: -0.262. Test r2: -0.346\n",
      "Epoch [15/20]. Step [81/121]. Loss: 0.010. Train r2: -0.384. Test r2: -0.325\n",
      "Epoch [15/20]. Step [121/121]. Loss: 0.010. Train r2: -0.389. Test r2: -0.327\n",
      "Epoch [16/20]. Step [1/121]. Loss: 0.012. Train r2: -0.018. Test r2: -0.326\n",
      "Epoch [16/20]. Step [41/121]. Loss: 0.010. Train r2: -0.401. Test r2: -0.336\n",
      "Epoch [16/20]. Step [81/121]. Loss: 0.010. Train r2: -0.224. Test r2: -0.322\n",
      "Epoch [16/20]. Step [121/121]. Loss: 0.010. Train r2: -0.364. Test r2: -0.317\n",
      "Epoch [17/20]. Step [1/121]. Loss: 0.011. Train r2: -0.062. Test r2: -0.335\n",
      "Epoch [17/20]. Step [41/121]. Loss: 0.010. Train r2: -0.387. Test r2: -0.325\n",
      "Epoch [17/20]. Step [81/121]. Loss: 0.011. Train r2: -0.355. Test r2: -0.317\n",
      "Epoch [17/20]. Step [121/121]. Loss: 0.010. Train r2: -0.239. Test r2: -0.329\n",
      "Epoch [18/20]. Step [1/121]. Loss: 0.011. Train r2: -0.004. Test r2: -0.329\n",
      "Epoch [18/20]. Step [41/121]. Loss: 0.010. Train r2: -0.349. Test r2: -0.325\n",
      "Epoch [18/20]. Step [81/121]. Loss: 0.010. Train r2: -0.338. Test r2: -0.361\n",
      "Epoch [18/20]. Step [121/121]. Loss: 0.010. Train r2: -0.287. Test r2: -0.334\n",
      "Epoch [19/20]. Step [1/121]. Loss: 0.010. Train r2: -0.007. Test r2: -0.333\n",
      "Epoch [19/20]. Step [41/121]. Loss: 0.011. Train r2: -0.369. Test r2: -0.317\n",
      "Epoch [19/20]. Step [81/121]. Loss: 0.010. Train r2: -0.372. Test r2: -0.318\n",
      "Epoch [19/20]. Step [121/121]. Loss: 0.010. Train r2: -0.608. Test r2: -0.339\n",
      "Epoch [20/20]. Step [1/121]. Loss: 0.012. Train r2: -0.003. Test r2: -0.337\n",
      "Epoch [20/20]. Step [41/121]. Loss: 0.010. Train r2: -0.615. Test r2: -0.341\n",
      "Epoch [20/20]. Step [81/121]. Loss: 0.010. Train r2: -0.352. Test r2: -0.320\n",
      "Epoch [20/20]. Step [121/121]. Loss: 0.010. Train r2: -0.327. Test r2: -0.331\n",
      "Обучение закончено!\n"
     ]
    }
   ],
   "source": [
    "# обучение нейросети: \n",
    "\n",
    "Adam_opt = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "RMSProp_opt = torch.optim.RMSprop(net.parameters(), lr=0.0001)\n",
    "SGD_opt = torch.optim.SGD(net.parameters(), lr=0.0001)\n",
    "\n",
    "optimizers = [Adam_opt, RMSProp_opt, SGD_opt]\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    print('_________________________________')\n",
    "    print('Оптимизатор: ', optimizer)\n",
    "    print('Обучение: ')\n",
    "    \n",
    "    num_epochs = 20\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss, running_items, r2 = 0.0, 0.0, 0.0\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data[0], data[1]\n",
    "\n",
    "            # обнуляю градиент: \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)                # предсказание\n",
    "            loss = criterion(outputs, labels)    # ошибка\n",
    "            loss.backward()                      # градиенты\n",
    "            optimizer.step()                     # шаг оптимизации\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_items += len(labels)\n",
    "\n",
    "            predict = outputs.data.numpy()\n",
    "            train_target = labels.view(labels.shape[0], 1).numpy()\n",
    "            r2 += r2_score(train_target, predict)\n",
    "\n",
    "            # вывожу статистику о процессе обучения: \n",
    "            if i % 40 == 0:\n",
    "                net.eval()\n",
    "\n",
    "                data = list(test_loader)[0]\n",
    "\n",
    "                test_outputs = net(data[0])\n",
    "                test_predict = test_outputs.data.numpy()\n",
    "                test_target = data[1].view(data[1].shape[0], 1)\n",
    "                test_r2 = r2_score(test_target, test_predict)\n",
    "\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
    "                      f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
    "                      f'Loss: {running_loss / running_items:.3f}. ' \\\n",
    "                      f'Train r2: {r2:.3f}. ' \\\n",
    "                      f'Test r2: {test_r2:.3f}')\n",
    "\n",
    "                running_loss, running_items, r2 = 0.0, 0.0, 0.0\n",
    "\n",
    "                net.train()\n",
    "\n",
    "    print('Обучение закончено!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейросеть с оптимизатором Adam показала более высокие результаты. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
